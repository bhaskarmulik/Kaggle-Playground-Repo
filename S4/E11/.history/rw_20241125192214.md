# Rough work

- There are rows with the same profession as the name. We need to remove these
- Check whether there are enough records to train the model
- Make sure to use redundancy for drop function
- Dealing with missing vals:
  - Create a temp df with no missing values
  - We can then analyse this data to make a simple regression model for the profession corresponding to the missing value
  - We also need to check correlations too

#### Thinking of using a embedding model for profession

Here is a step-by-step plan to achieve this:

- Prepare the Data: Convert the 'Profession' column to a categorical type and then to integer codes.
- Create an Embedding Layer: Define an embedding layer in your neural network to learn the embeddings for the 'Profession' feature.
- Train the Neural Network: Train the neural network with the embedding layer included.

#### Dealing with missing values

##### **Advanced Techniques for Dealing with Missing Values**

###### 1. **Predictive Modeling (Imputation Using ML Models)**

* **Method** : Use machine learning algorithms to predict the missing values based on other features in the dataset.
* **Steps** :

1. Treat the feature with missing values as the target variable.
2. Train a regression (for numerical data) or classification model (for categorical data) using other features as predictors.
3. Predict missing values using the trained model.

* **Algorithms** : Random Forest, Gradient Boosting (XGBoost, LightGBM), K-Nearest Neighbors, etc.
* **Advantages** :
* Leverages relationships between variables.
* Handles both numerical and categorical data effectively.
* **Disadvantages** :
* Computationally intensive.
* Requires a separate model for each feature with missing values.

---

###### 2. **K-Nearest Neighbors (KNN) Imputation**

* **Method** : For each missing value, find the k-nearest rows (using a distance metric like Euclidean or cosine distance) and use their values to impute.
* **Advantages** :
* Captures local relationships in the data.
* **Disadvantages** :
* Computationally expensive for large datasets.
* Sensitive to the choice of k and distance metric.

---

###### 3. **Multivariate Imputation by Chained Equations (MICE)**

* **Method** : Iteratively predicts missing values for each feature based on all other features in the dataset using regression models.
* **Advantages** :
* Captures multivariate relationships.
* Produces multiple imputations for uncertainty estimation.
* **Disadvantages** :
* Computationally intensive.
* Requires careful tuning of iteration parameters.

---

#### 4. **Matrix Factorization (e.g., SVD, PCA)**

* **Method** : Uses techniques like Singular Value Decomposition (SVD) or Principal Component Analysis (PCA) to approximate the data matrix and fill in missing values.
* **Advantages** :
* Suitable for large-scale data.
* Preserves global structure.
* **Disadvantages** :
* Assumes linear relationships between variables.
* May not work well if missing values are sparse.

---

#### 5. **Generative Models**

* **Examples** :
* **Autoencoders** : Train an autoencoder to reconstruct data and use the reconstructed outputs for imputation.
* **Generative Adversarial Networks (GANs)** : Use GANs to generate missing data points by learning the distribution of the dataset.
* **Advantages** :
* Handles complex relationships.
* Can generate realistic imputations.
* **Disadvantages** :
* Requires substantial computational resources and tuning.
* Can overfit if not carefully regularized.

---

#### 6. **Clustering-Based Imputation**

* **Method** : Use clustering algorithms (e.g., K-means, DBSCAN) to group similar data points and impute missing values based on cluster averages or modes.
* **Advantages** :
* Captures local patterns.
* **Disadvantages** :
* Requires pre-clustering of the data.
* Sensitive to the quality of clustering.

---

#### 7. **Time-Series Specific Imputation**

* For missing values in time-series data:
  * **Interpolation** : Linear, cubic, or spline interpolation based on adjacent timestamps.
  * **Kalman Filtering** : Estimates missing values using a dynamic state-space model.
  * **State-Space Models** : Utilize statistical models like ARIMA or Seasonal Decomposition to impute.
  * **Deep Learning** : Use recurrent neural networks (RNNs) or transformers to predict missing sequences.

---

#### 8. **Bayesian Imputation**

* **Method** : Use Bayesian models to impute missing values by treating them as latent variables and sampling from the posterior distribution.
* **Advantages** :
* Accounts for uncertainty in imputations.
* **Disadvantages** :
* Computationally intensive.
* Requires expertise in Bayesian modeling.

---

#### 9. **Hot Deck Imputation**

* **Method** : Impute missing values using observed values from similar cases (e.g., matching based on other variables).
* **Advantages** :
* Simple and intuitive.
* **Disadvantages** :
* Requires a good matching mechanism.

---

#### 10. **Domain-Specific Methods**

* Incorporate domain knowledge to infer missing values.
* Examples:
  * In healthcare, impute based on medical guidelines.
  * In finance, use historical trends or averages specific to sectors.

---

### **Considerations**

* **Dataset Characteristics** :
* For small datasets, simpler methods like KNN or Hot Deck might suffice.
* For large datasets, advanced methods like MICE, autoencoders, or GANs are more appropriate.
* **Missing Data Mechanism** :
* Consider whether the data is Missing Completely at Random (MCAR), Missing at Random (MAR), or Missing Not at Random (MNAR). Some methods (e.g., Bayesian imputation) are better suited for MAR or MNAR
